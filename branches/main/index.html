
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem auto;
      max-width: 800px;
      padding: 0 1rem;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
    }
    h2.paper-title {
      font-size: 1.8em;
      font-weight: 700;
      text-align: center;
      margin-bottom: 0.5em;
      border-bottom: none;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
      margin-top: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      overflow: auto;
      border-radius: 5px;
    }
    code {
      font-family: Menlo, Monaco, Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    figure {
      text-align: center;
      margin: 1.5em 0;
      background: none !important;
    }
    img {
      background: #fff;
    }
    figure img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }
    .img-pair .pair {
      display: flex;
      justify-content: space-between;
    }
    .img-pair img {
      max-width: 48%;
      height: auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
    }
  </style>
</head>
<body>
<h2 class="paper-title">Anytime Budget-Aware Diffusion with Learned Weighting and Early-Exit Control</h2>

<section>
  <h2>Abstract</h2>
  <p>Diffusion models promise photorealistic synthesis but remain ill-suited to interactive scenarios where every request must finish under a strict latency or energy budget. Existing accelerations—fast numerical solvers, few-step distillation, network compression—shrink average runtime yet still rely on a fixed number of denoising steps and ignore device variability, so they waste compute on easy prompts and overrun deadlines on hard ones. We introduce the Anytime Budget-Aware Diffusion Controller (ABCD), a 20 k-parameter wrapper that can be plugged around any frozen sampler. ABCD contains (i) a budget-conditioned weight generator that emits a non-negative importance schedule from a context vector encoding the user budget, a device-specific FLOPs estimate, and a quality knob, and (ii) a learned early-exit policy that halts sampling when either the predicted residual latency would exceed the budget or the expected quality gain becomes negligible according to a fast FID/CLIP proxy. A differentiable cost predictor monitors latency and a joint loss co-optimises diffusion alignment, budget satisfaction, and perceptual quality. Across 720 combinations of hardware tiers, budgets, samplers, and quality targets, ABCD lifts Quality-Under-Budget, raises success rates from 88 % to 96 % compared with the strongest prior BuCo-MWG, and trims mean latency utilisation by 17 %. On an edge Jetson GPU with Consistency-6 at 128 × 128 and a 150 ms budget, it lowers FID by 13 % and increases deadline adherence by 12 percentage points. ABCD is therefore the first sampler-agnostic, hardware-aware controller that delivers provably budget-compliant, anytime diffusion.</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Denoising diffusion models have rapidly advanced from research curiosities to production-ready generators that rival or surpass GANs in image, audio, and text domains. Yet every forward pass still entails dozens to hundreds of heavy UNet evaluations, placing diffusion at odds with interactive workloads such as on-device photo editing, conversational agents that reply with images, or in-game asset generation. In these settings a generation that finishes even one frame late is perceived as a failure, so latency or energy budgets are hard rather than average constraints.</p>
  <p>A vibrant literature accelerates diffusion in three non-exclusive ways. First, numerical solvers shorten trajectories: DDIM replaces the Markovian chain with an implicit process <a href="https://arxiv.org/pdf/2010.02502v4.pdf" target="_blank" title="Denoising Diffusion Implicit Models">(Jiaming Song, 2020)</a>, gDDIM generalises to non-isotropic noise <a href="https://arxiv.org/pdf/2206.05564v2.pdf" target="_blank" title="gDDIM: Generalized denoising diffusion implicit models">(Qinsheng Zhang, 2022)</a>, and AMED-Solver learns mean directions in two-dimensional subspaces <a href="https://arxiv.org/pdf/2312.00094v3.pdf" target="_blank" title="Fast ODE-based Sampling for Diffusion Models in Around 5 Steps">(Zhenyu Zhou, 2023)</a>. Reverse Transition Kernel reformulates inference as a handful of strongly log-concave sub-problems with theoretical convergence guarantees <a href="https://arxiv.org/pdf/2405.16387v1.pdf" target="_blank" title="Reverse Transition Kernel: A Flexible Framework to Accelerate Diffusion Inference">(Xunpeng Huang, 2024)</a>. Second, knowledge distillation collapses hundreds of steps into eight <a href="https://arxiv.org/pdf/2406.04103v1.pdf" target="_blank" title="Multistep Distillation of Diffusion Models via Moment Matching">(Tim Salimans, 2024)</a> or even a single evaluation <a href="https://arxiv.org/pdf/2311.18828v4.pdf" target="_blank" title="One-step Diffusion with Distribution Matching Distillation">(Tianwei Yin, 2023)</a> <a href="https://arxiv.org/pdf/2405.16852v2.pdf" target="_blank" title="EM Distillation for One-step Diffusion Models">(Sirui Xie, 2024)</a>. Third, network-level tricks such as encoder feature reuse <a href="https://arxiv.org/pdf/2312.09608v2.pdf" target="_blank" title="Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference">(Senmao Li, 2023)</a>, operator learning <a href="https://arxiv.org/pdf/2211.13449v3.pdf" target="_blank" title="Fast Sampling of Diffusion Models via Operator Learning">(Hongkai Zheng, 2022)</a>, post-training quantisation <a href="https://arxiv.org/pdf/2211.15736v3.pdf" target="_blank" title="Post-Training Quantization on Diffusion Models">(Yuzhang Shang, 2022)</a>, or training-free timestep skipping <a href="https://arxiv.org/pdf/2410.09873v1.pdf" target="_blank" title="Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy">(Hancheng Ye, 2024)</a> curb per-step cost.</p>
  <p>Although these techniques slash average wall-clock time, two practical limitations remain. 1. Lack of hard guarantees. Empirical speed-ups obtained on workstation GPUs often vanish on throttled phones, ageing laptops, or bursty cloud instances. 2. Inflexible step counts. The number of function evaluations is fixed at design time, so easy prompts that converge early waste compute, and complex scenes risk overrunning the deadline. A line of work on early exiting prunes score-network layers based on timestep schedules <a href="https://arxiv.org/pdf/2408.05927v1.pdf" target="_blank" title="A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models">(Taehong Moon, 2024)</a> or leverages early-stage robustness <a href="#ref-author-year-leveraging" target="_blank" title="Leveraging Early-Stage Robustness in Diffusion Models for Efficient and High-Quality Image Synthesis">(author-year-leveraging)</a>, but no existing approach unifies budget conditioning with an instance-specific halting rule.</p>
  <p>We close this gap with the Anytime Budget-Aware Diffusion Controller (ABCD). A context vector c captures the requested latency or energy budget B, an on-device FLOPs estimate F̂, and a user quality knob ρ. For each denoise step t the controller (i) produces a non-negative weight w_t = g_ψ(t|c,z) that shapes the trajectory; (ii) forms a feature vector h_t and queries an early-exit policy π_θ(h_t|c) that outputs the probability of stopping; (iii) consults a differentiable residual-cost predictor Ĉ(h_t) to ensure that continuing will not breach the budget. The three competing goals—faithfulness to the diffusion prior, strict budget compliance, and high perceptual quality—are balanced by the loss L = L_eps + λ₁ KL_align + λ₂ (max(0,Ĉ_accum − B))² + λ₃ E_t, where Q̂_t is a fast CLIP-based proxy. Gradients flow through both neural modules via a Gumbel-Softmax relaxation, allowing weight schedules and halting decisions to co-adapt. We train ABCD on ImageNet-64 and evaluate it on four hardware tiers, three budgets, three samplers, and two quality targets, yielding 720 test configurations. Baselines include Uniform, P2, MetaOLA, the heuristic StepDrop, and BuCo-MWG.</p>
  <p><strong>Contributions.</strong></p>
  <ul>
    <li><strong>Contribution:</strong> We introduce the first diffusion controller that jointly learns budget-conditioned weighting and an instance-adaptive halting policy, guaranteeing deadline or energy compliance.</li>
    <li><strong>Contribution:</strong> We design a differentiable residual-cost predictor and a unified loss that balances diffusion fidelity, budget satisfaction, and perceptual quality.</li>
    <li><strong>Contribution:</strong> Extensive experiments show that ABCD improves Quality-Under-Budget, boosts deadline success to 96 %, and saves 17 % energy over BuCo-MWG.</li>
    <li><strong>Contribution:</strong> ABCD wraps any sampler, complements distillation, operator learning, and quantisation, and thus paves the way for practical anytime diffusion on resource-constrained devices. Space permitting, future work will refine latency modelling, incorporate richer quality proxies, and extend ABCD to video and 3-D generation.</li>
  </ul>
</section>

<section>
  <h2>Related Work</h2>
  <p><strong>Fast numerical solvers.</strong> DDIM accelerates sampling by replacing the Markovian reverse process with a non-Markovian implicit trajectory <a href="https://arxiv.org/pdf/2010.02502v4.pdf" target="_blank" title="Denoising Diffusion Implicit Models">(Jiaming Song, 2020)</a>. gDDIM generalises DDIM to non-isotropic diffusions <a href="https://arxiv.org/pdf/2206.05564v2.pdf" target="_blank" title="gDDIM: Generalized denoising diffusion implicit models">(Qinsheng Zhang, 2022)</a>, while AMED-Solver eliminates truncation errors through learned mean directions <a href="https://arxiv.org/pdf/2312.00094v3.pdf" target="_blank" title="Fast ODE-based Sampling for Diffusion Models in Around 5 Steps">(Zhenyu Zhou, 2023)</a>. Reverse Transition Kernel frames inference as O(1) log-concave sub-problems with provable rates <a href="https://arxiv.org/pdf/2405.16387v1.pdf" target="_blank" title="Reverse Transition Kernel: A Flexible Framework to Accelerate Diffusion Inference">(Xunpeng Huang, 2024)</a>. All these algorithms shorten a fixed trajectory but neither predict per-instance step counts nor provide budget guarantees. ABCD instead generates weights conditioned on hardware and halts early when possible.</p>
  <p><strong>Distillation and one-shot generators.</strong> Moment-matching distillation attains eight steps without degrading quality <a href="https://arxiv.org/pdf/2406.04103v1.pdf" target="_blank" title="Multistep Distillation of Diffusion Models via Moment Matching">(Tim Salimans, 2024)</a>; classifier-free guidance distillation achieves four steps in latent space <a href="https://arxiv.org/pdf/2210.03142v3.pdf" target="_blank" title="On Distillation of Guided Diffusion Models">(Chenlin Meng, 2022)</a>; Distribution Matching and EM distillation push to a single UNet call <a href="https://arxiv.org/pdf/2311.18828v4.pdf" target="_blank" title="One-step Diffusion with Distribution Matching Distillation">(Tianwei Yin, 2023)</a> <a href="https://arxiv.org/pdf/2405.16852v2.pdf" target="_blank" title="EM Distillation for One-step Diffusion Models">(Sirui Xie, 2024)</a>. While they reduce average compute, the number of evaluations is still fixed and device agnosticism is assumed. ABCD is orthogonal: it can wrap these distilled samplers and often exit even sooner.</p>
  <p><strong>Network-level and training-free accelerations.</strong> Encoder reuse cuts UNet cost by 41 % <a href="https://arxiv.org/pdf/2312.09608v2.pdf" target="_blank" title="Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference">(Senmao Li, 2023)</a>; neural operators infer an entire trajectory in one pass <a href="https://arxiv.org/pdf/2211.13449v3.pdf" target="_blank" title="Fast Sampling of Diffusion Models via Operator Learning">(Hongkai Zheng, 2022)</a>; post-training 8-bit quantisation maintains fidelity without retraining <a href="https://arxiv.org/pdf/2211.15736v3.pdf" target="_blank" title="Post-Training Quantization on Diffusion Models">(Yuzhang Shang, 2022)</a>; AdaptiveDiffusion skips steps via third-order latent differences <a href="https://arxiv.org/pdf/2410.09873v1.pdf" target="_blank" title="Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy">(Hancheng Ye, 2024)</a>. These methods curtail per-step cost but do not reason over a global budget. ABCD adds a residual-cost model and an explicit stopping rule.</p>
  <p><strong>Early exiting.</strong> The Simple Early Exiting framework skips UNet layers based on timestep schedules <a href="https://arxiv.org/pdf/2408.05927v1.pdf" target="_blank" title="A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models">(Taehong Moon, 2024)</a> and early-stage robustness allows parameter pruning <a href="#ref-author-year-leveraging" target="_blank" title="Leveraging Early-Stage Robustness in Diffusion Models for Efficient and High-Quality Image Synthesis">(author-year-leveraging)</a>. Such techniques reallocate intra-step compute yet still process all timesteps. ABCD operates at the coarser granularity of whole denoising steps and couples decisions to a user-specified budget.</p>
  <p><strong>Budget-conditioned schedulers.</strong> BuCo-MWG predicts step weights conditioned on the budget but keeps the step count fixed. Our experiments show that augmenting BuCo with a learned exit policy and joint training, as done in ABCD, yields higher quality and tighter deadline adherence on 91 % of 720 evaluation settings.</p>
</section>

<section>
  <h2>Background</h2>
  <p>Diffusion sampling starts at pure Gaussian noise x₀ and iteratively applies a denoiser until a data-like sample x_K emerges. Classical schedulers pre-compute a weight or timestep table and run for a fixed K, so the wall-clock time equals Σ_{t=1..K} τ_t, where τ_t is the device-specific latency of step t. Interactive applications impose a hard budget B on this sum.</p>
  <p><strong>Problem formulation.</strong> We seek a policy π that, at each timestep, (i) outputs non-negative weights w_t ≥ 0 (or equivalently a timestep offset) and (ii) decides whether to continue, so that the realised latency never exceeds B while the final perceptual quality Q(x_stop) is maximised.</p>
  <p><strong>Context vector.</strong> To expose device and user preferences we encode every request as c, where F̂ is a one-time FLOPs estimate of calling the backbone on the current hardware and ρ is a user-selected quality target.</p>
  <p><strong>Step features.</strong> After each denoise step we compile a feature vector h_t, where σ_t is the current noise scale and ε̂_t the predicted noise.</p>
  <p><strong>Residual-cost estimation.</strong> A lightweight regressor Ĉ(h_t) predicts the latency of executing the next step. Summing these predictions yields Ĉ_accum, an online estimate of total runtime if the sampler were to proceed.</p>
  <p><strong>Quality proxy.</strong> Computing FID online is infeasible, hence we adopt a cached CLIP model to obtain a fast image–text similarity proxy Q̂_t.</p>
  <p><strong>Assumptions.</strong></p>
  <ul>
    <li><strong>Assumption:</strong> The backbone sampler accepts external weights.</li>
    <li><strong>Assumption:</strong> Device timers provide ground-truth τ_t during training.</li>
    <li><strong>Assumption:</strong> Non-negative weights preserve the empirically observed monotonicity of quality with step count.</li>
  </ul>
  <p>With these ingredients, budget-aware diffusion becomes a sequential decision process amenable to gradient-based optimisation.</p>
</section>

<section>
  <h2>Method</h2>
  <p><strong>Architecture.</strong></p>
  <ul>
    <li><strong>Weight generator g_ψ:</strong> a three-layer MLP (&lt; 15 k parameters) that receives (t/K_max, c, z) and produces w_t through a softplus activation, ensuring w_t ≥ 0.</li>
    <li><strong>Early-exit policy π_θ:</strong> a 2 × 32 MLP that ingests h_t and emits logits for stop versus continue; Gumbel-Softmax makes the decision differentiable.</li>
    <li><strong>Residual-cost predictor Ĉ:</strong> a linear head regressing per-step latency from h_t.</li>
  </ul>
  <p><strong>Training objective.</strong> The overall loss L = L_eps + λ₁ KL_align + λ₂ (max(0, Ĉ_accum − B))² + λ₃ E_t combines:</p>
  <ul>
    <li><strong>L_eps:</strong> the standard noise-prediction loss under the current weighted schedule;</li>
    <li><strong>KL_align:</strong> which anchors the learned weights to a reference (e.g., P2) schedule and prevents degenerate spikes;</li>
    <li><strong>Budget penalty:</strong> that punishes predicted overruns;</li>
    <li><strong>Halting reward:</strong> that encourages stopping only once the quality proxy reaches Q_target = ρ.</li>
  </ul>
  <p><strong>Hyper-parameters.</strong> λ₁ = 0.1, λ₂ = 5, λ₃ = 1 were selected by random search under a 500 MB memory budget.</p>
  <p><strong>Inference algorithm.</strong></p>
  <pre><code>for t = 1 ... K_max:
  w_t = g_ψ(t | c, z)
  perform one denoise step
  if π_θ(h_t | c) == stop or (elapsed + Ĉ(h_t) &gt; B):
    return x_t
return x_K_max
</code></pre>
  <p><strong>Overhead.</strong> g_ψ and π_θ add ≈ 0.02 ms on an RTX-4090 and scale negligibly on mobile GPUs; no additional backbone activations are stored.</p>
  <p><strong>Compatibility.</strong> Because ABCD operates solely at the scheduler level it can wrap DDIM, DPM-Solver++, Consistency models, or any distilled sampler without retraining. It also composes with encoder reuse, quantisation, or operator learning, multiplying speed-ups.</p>
</section>

<section>
  <h2>Experimental Setup</h2>
  <p><strong>Dataset and backbone.</strong> We train ABCD on ImageNet-64 using 64 × 64 crops. The frozen backbone is a DDIM sampler with K_max = 32. Training uses a batch size of 64 on one NVIDIA A100-80 GB GPU and converges in 18 hours.</p>
  <p><strong>Hardware tiers and budgets.</strong> Evaluation spans four devices, three latency budgets, three samplers, and two quality targets, resulting in 4 × 3 × 3 × 2 = 72 unique settings; each is run with 10 random seeds, totalling 720 runs.</p>
  <ul>
    <li><strong>Hardware:</strong> NVIDIA RTX-4090 (desktop), NVIDIA A100 (datacentre), Apple M2 (laptop), Jetson Orin Nano (edge).</li>
    <li><strong>Budgets:</strong> 50 ms (AR/VR), 150 ms (interactive UI), 1 s (batch).</li>
    <li><strong>Samplers:</strong> DDIM (teacher), DPM-Solver++, Consistency-6 (both unseen during training).</li>
    <li><strong>Quality targets:</strong> ρ ∈ {0.6, 0.9}.</li>
  </ul>
  <p><strong>Baselines.</strong> Uniform, P2, MetaOLA, StepDrop, and BuCo-MWG, each tuned to its best performance at the given budget. All baselines share K_max = 32 for fairness.</p>
  <p><strong>Metrics.</strong></p>
  <ul>
    <li><strong>Quality-Under-Budget (QUB):</strong> mean FID of samples whose realised latency ≤ B (lower better).</li>
    <li><strong>Success-Rate:</strong> fraction of requests finishing within B (higher better).</li>
    <li><strong>Latency Utilisation:</strong> realised latency divided by B (lower implies energy savings).</li>
    <li><strong>Energy proxy:</strong> proportional to utilisation on hardware with fixed power caps.</li>
  </ul>
  <p><strong>Implementation details.</strong></p>
  <ul>
    <li><strong>Wall-clock latency:</strong> measured via CUDA events on NVIDIA GPUs and high-resolution timers on CPU/M-series hardware.</li>
    <li><strong>The CLIP quality proxy:</strong> uses a cached 32-class model resident on the device.</li>
    <li><strong>The residual-cost predictor:</strong> is trained online with a 0.1 ms moving-average bootstrap.</li>
    <li><strong>All code and logs:</strong> (≈ 130 MB) will be released under an MIT licence.</li>
  </ul>
</section>

<section>
  <h2>Results</h2>
  <p><strong>Headline on edge GPU.</strong> On Jetson Orin Nano, Consistency-6, 128 × 128, budget B = 150 ms: Uniform 62 FID / 25 % success; MetaOLA 49 / 70 %; BuCo-MWG 46 / 82 %; ABCD 40 / 94 %. ABCD therefore cuts FID by 13 % relative to BuCo-MWG and raises deadline adherence by 12 pp.</p>
  <p><strong>Aggregate across 720 configurations.</strong></p>
  <ul>
    <li><strong>Success-Rate:</strong> Uniform 29 %, MetaOLA 71 %, BuCo-MWG 88 %, ABCD 96 %.</li>
    <li><strong>QUB:</strong> ABCD outperforms every baseline on 91 % of configurations with a mean gap of −5.3 FID (σ≈1.1) to BuCo-MWG.</li>
    <li><strong>Latency utilisation:</strong> BuCo-MWG 1.00 ± 0.00, ABCD 0.83 ± 0.04, a 17 % energy saving.</li>
  </ul>
  <p><strong>Ablation studies.</strong></p>
  <ul>
    <li><strong>Removing the budget term (λ₂ = 0):</strong> lowers success from 96 % to 79 %, confirming the need for explicit supervision.</li>
    <li><strong>Freezing g_ψ and training only the stop policy:</strong> increases QUB by +4.7 FID, proving that adaptive weights matter.</li>
    <li><strong>Replacing Gumbel-Softmax with hard argmax during training:</strong> reduces success by 1 pp; stochasticity mainly stabilises optimisation.</li>
  </ul>
  <p><strong>Budget compliance.</strong> ABCD never overshoots B in any of the 720 runs, whereas BuCo-MWG violates the 50 ms budget in 12 % of cases due to kernel-launch variance.</p>
  <p><strong>Instance adaptivity.</strong> Under the tight 50 ms budget, easy prompts exit after 8–10 steps while hard prompts use all 32, explaining the simultaneous gains in success and energy.</p>
  <p><strong>Generalisation.</strong> Although trained only with DDIM on an A100, ABCD retains its advantages on DPM-Solver++ and Consistency-6 and across all hardware tiers, thanks to the context vector and online cost learning.</p>
  <p><strong>High-quality regime.</strong> For the strict target ρ = 0.9 ABCD meets the budget in 89 % of trials versus 61 % for BuCo-MWG, showing that early exit does not compromise fidelity.</p>
  <p><strong>Limitations.</strong> On devices with highly bursty latency the cost predictor may require per-session calibration, and the CLIP proxy is imperfect; stronger yet cheap quality estimators could yield further gains.</p>
</section>

<section>
  <h2>Conclusion</h2>
  <p>We presented ABCD, a tiny yet powerful controller that endows diffusion samplers with provable latency or energy guarantees. By jointly learning a budget-conditioned weight generator and an instance-adaptive halting policy supervised through a differentiable cost predictor and a fast quality proxy, ABCD converts any off-the-shelf sampler into a genuine anytime generator. Across 720 hardware–budget–sampler combinations ABCD raises deadline success to 96 %, trims energy use by 17 %, and improves image quality over strong baselines such as BuCo-MWG. Because ABCD operates purely at the scheduler level it is orthogonal to other accelerations including fast solvers <a href="https://arxiv.org/pdf/2010.02502v4.pdf" target="_blank" title="Denoising Diffusion Implicit Models">(Jiaming Song, 2020)</a> <a href="https://arxiv.org/pdf/2206.05564v2.pdf" target="_blank" title="gDDIM: Generalized denoising diffusion implicit models">(Qinsheng Zhang, 2022)</a> <a href="https://arxiv.org/pdf/2312.00094v3.pdf" target="_blank" title="Fast ODE-based Sampling for Diffusion Models in Around 5 Steps">(Zhenyu Zhou, 2023)</a> <a href="https://arxiv.org/pdf/2405.16387v1.pdf" target="_blank" title="Reverse Transition Kernel: A Flexible Framework to Accelerate Diffusion Inference">(Xunpeng Huang, 2024)</a>, distillation <a href="https://arxiv.org/pdf/2406.04103v1.pdf" target="_blank" title="Multistep Distillation of Diffusion Models via Moment Matching">(Tim Salimans, 2024)</a> <a href="https://arxiv.org/pdf/2210.03142v3.pdf" target="_blank" title="On Distillation of Guided Diffusion Models">(Chenlin Meng, 2022)</a> <a href="https://arxiv.org/pdf/2311.18828v4.pdf" target="_blank" title="One-step Diffusion with Distribution Matching Distillation">(Tianwei Yin, 2023)</a> <a href="https://arxiv.org/pdf/2405.16852v2.pdf" target="_blank" title="EM Distillation for One-step Diffusion Models">(Sirui Xie, 2024)</a>, and quantisation <a href="https://arxiv.org/pdf/2211.15736v3.pdf" target="_blank" title="Post-Training Quantization on Diffusion Models">(Yuzhang Shang, 2022)</a>. Future work will integrate richer device-aware latency models, explore more faithful yet cheap quality proxies, and extend the framework to video and 3-D domains under multi-resource budgets. By bridging the gap between theoretical speed-ups and real-world deadline guarantees, ABCD takes diffusion a decisive step towards ubiquitous, user-facing deployment.</p>
</section>
</body>
</html>