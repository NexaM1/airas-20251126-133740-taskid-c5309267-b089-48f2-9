{
  "research_topic": "diffusion model speedup",
  "queries": [
    "fast diffusion sampling",
    "diffusion model acceleration",
    "denoising diffusion acceleration",
    "DDIM fast sampling",
    "diffusion model distillation"
  ],
  "research_study_list": [
    {
      "title": "Multistep Distillation of Diffusion Models via Moment Matching",
      "abstract": "We present a new method for making diffusion models faster to sample. The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory. Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching. By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset. We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers.",
      "meta_data": {
        "arxiv_id": "2406.04103v1",
        "authors": [
          "Tim Salimans",
          "Thomas Mensink",
          "Jonathan Heek",
          "Emiel Hoogeboom"
        ],
        "published_date": "2024-06-06T14:20:21Z",
        "pdf_url": "https://arxiv.org/pdf/2406.04103v1.pdf"
      }
    },
    {
      "title": "A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models",
      "abstract": "Diffusion models have shown remarkable performance in generation problems over various domains including images, videos, text, and audio. A practical bottleneck of diffusion models is their sampling speed, due to the repeated evaluation of score estimation networks during the inference. In this work, we propose a novel framework capable of adaptively allocating compute required for the score estimation, thereby reducing the overall sampling time of diffusion models. We observe that the amount of computation required for the score estimation may vary along the time step for which the score is estimated. Based on this observation, we propose an early-exiting scheme, where we skip the subset of parameters in the score estimation network during the inference, based on a time-dependent exit schedule. Using the diffusion models for image synthesis, we show that our method could significantly improve the sampling throughput of the diffusion models without compromising image quality. Furthermore, we also demonstrate that our method seamlessly integrates with various types of solvers for faster sampling, capitalizing on their compatibility to enhance overall efficiency. The source code and our experiments are available at \\url{https://github.com/taehong-moon/ee-diffusion}",
      "meta_data": {
        "arxiv_id": "2408.05927v1",
        "authors": [
          "Taehong Moon",
          "Moonseok Choi",
          "EungGu Yun",
          "Jongmin Yoon",
          "Gayoung Lee",
          "Jaewoong Cho",
          "Juho Lee"
        ],
        "published_date": "2024-08-12T05:33:45Z",
        "pdf_url": "https://arxiv.org/pdf/2408.05927v1.pdf"
      }
    },
    {
      "title": "Reverse Transition Kernel: A Flexible Framework to Accelerate Diffusion Inference",
      "abstract": "To generate data from trained diffusion models, most inference algorithms, such as DDPM, DDIM, and other variants, rely on discretizing the reverse SDEs or their equivalent ODEs. In this paper, we view such approaches as decomposing the entire denoising diffusion process into several segments, each corresponding to a reverse transition kernel (RTK) sampling subproblem. Specifically, DDPM uses a Gaussian approximation for the RTK, resulting in low per-subproblem complexity but requiring a large number of segments (i.e., subproblems), which is conjectured to be inefficient. To address this, we develop a general RTK framework that enables a more balanced subproblem decomposition, resulting in $\\tilde O(1)$ subproblems, each with strongly log-concave targets. We then propose leveraging two fast sampling algorithms, the Metropolis-Adjusted Langevin Algorithm (MALA) and Underdamped Langevin Dynamics (ULD), for solving these strongly log-concave subproblems. This gives rise to the RTK-MALA and RTK-ULD algorithms for diffusion inference. In theory, we further develop the convergence guarantees for RTK-MALA and RTK-ULD in total variation (TV) distance: RTK-ULD can achieve $ε$ target error within $\\tilde{\\mathcal O}(d^{1/2}ε^{-1})$ under mild conditions, and RTK-MALA enjoys a $\\mathcal{O}(d^{2}\\log(d/ε))$ convergence rate under slightly stricter conditions. These theoretical results surpass the state-of-the-art convergence rates for diffusion inference and are well supported by numerical experiments.",
      "meta_data": {
        "arxiv_id": "2405.16387v1",
        "authors": [
          "Xunpeng Huang",
          "Difan Zou",
          "Hanze Dong",
          "Yi Zhang",
          "Yi-An Ma",
          "Tong Zhang"
        ],
        "published_date": "2024-05-26T00:26:57Z",
        "pdf_url": "https://arxiv.org/pdf/2405.16387v1.pdf"
      }
    },
    {
      "title": "Fast ODE-based Sampling for Diffusion Models in Around 5 Steps",
      "abstract": "Sampling from diffusion models can be treated as solving the corresponding ordinary differential equations (ODEs), with the aim of obtaining an accurate solution with as few number of function evaluations (NFE) as possible. Recently, various fast samplers utilizing higher-order ODE solvers have emerged and achieved better performance than the initial first-order one. However, these numerical methods inherently result in certain approximation errors, which significantly degrades sample quality with extremely small NFE (e.g., around 5). In contrast, based on the geometric observation that each sampling trajectory almost lies in a two-dimensional subspace embedded in the ambient space, we propose Approximate MEan-Direction Solver (AMED-Solver) that eliminates truncation errors by directly learning the mean direction for fast diffusion sampling. Besides, our method can be easily used as a plugin to further improve existing ODE-based samplers. Extensive experiments on image synthesis with the resolution ranging from 32 to 512 demonstrate the effectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10, 10.74 FID on ImageNet 64$\\times$64, and 13.20 FID on LSUN Bedroom. Our code is available at https://github.com/zju-pi/diff-sampler.",
      "meta_data": {
        "arxiv_id": "2312.00094v3",
        "authors": [
          "Zhenyu Zhou",
          "Defang Chen",
          "Can Wang",
          "Chun Chen"
        ],
        "published_date": "2023-11-30T13:07:19Z",
        "pdf_url": "https://arxiv.org/pdf/2312.00094v3.pdf"
      }
    },
    {
      "title": "Semi-Implicit Denoising Diffusion Models (SIDDMs)",
      "abstract": "Despite the proliferation of generative models, achieving fast sampling during inference without compromising sample diversity and quality remains challenging. Existing models such as Denoising Diffusion Probabilistic Models (DDPM) deliver high-quality, diverse samples but are slowed by an inherently high number of iterative steps. The Denoising Diffusion Generative Adversarial Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN model for larger jumps in the diffusion process. However, DDGAN encountered scalability limitations when applied to large datasets. To address these limitations, we introduce a novel approach that tackles the problem by matching implicit and explicit factors. More specifically, our approach involves utilizing an implicit model to match the marginal distributions of noisy data and the explicit conditional distribution of the forward diffusion. This combination allows us to effectively match the joint denoising distributions. Unlike DDPM but similar to DDGAN, we do not enforce a parametric distribution for the reverse step, enabling us to take large steps during inference. Similar to the DDPM but unlike DDGAN, we take advantage of the exact form of the diffusion process. We demonstrate that our proposed method obtains comparable generative performance to diffusion-based models and vastly superior results to models with a small number of sampling steps.",
      "meta_data": {
        "arxiv_id": "2306.12511v3",
        "authors": [
          "Yanwu Xu",
          "Mingming Gong",
          "Shaoan Xie",
          "Wei Wei",
          "Matthias Grundmann",
          "Kayhan Batmanghelich",
          "Tingbo Hou"
        ],
        "published_date": "2023-06-21T18:49:22Z",
        "pdf_url": "https://arxiv.org/pdf/2306.12511v3.pdf"
      }
    },
    {
      "title": "Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference",
      "abstract": "One of the main drawback of diffusion models is the slow inference time for image generation. Among the most successful approaches to addressing this problem are distillation methods. However, these methods require considerable computational resources. In this paper, we take another approach to diffusion model acceleration. We conduct a comprehensive study of the UNet encoder and empirically analyze the encoder features. This provides insights regarding their changes during the inference process. In particular, we find that encoder features change minimally, whereas the decoder features exhibit substantial variations across different time-steps. This insight motivates us to omit encoder computation at certain adjacent time-steps and reuse encoder features of previous time-steps as input to the decoder in multiple time-steps. Importantly, this allows us to perform decoder computation in parallel, further accelerating the denoising process. Additionally, we introduce a prior noise injection method to improve the texture details in the generated image. Besides the standard text-to-image task, we also validate our approach on other tasks: text-to-video, personalized generation and reference-guided generation. Without utilizing any knowledge distillation technique, our approach accelerates both the Stable Diffusion (SD) and DeepFloyd-IF model sampling by 41$\\%$ and 24$\\%$ respectively, and DiT model sampling by 34$\\%$, while maintaining high-quality generation performance.",
      "meta_data": {
        "arxiv_id": "2312.09608v2",
        "authors": [
          "Senmao Li",
          "Taihang Hu",
          "Joost van de Weijer",
          "Fahad Shahbaz Khan",
          "Tao Liu",
          "Linxuan Li",
          "Shiqi Yang",
          "Yaxing Wang",
          "Ming-Ming Cheng",
          "Jian Yang"
        ],
        "published_date": "2023-12-15T08:46:43Z",
        "pdf_url": "https://arxiv.org/pdf/2312.09608v2.pdf"
      }
    },
    {
      "title": "Leveraging Early-Stage Robustness in Diffusion Models for Efficient and High-Quality Image Synthesis"
    },
    {
      "title": "Post-Training Quantization on Diffusion Models",
      "abstract": "Denoising diffusion (score-based) generative models have recently achieved significant accomplishments in generating realistic and diverse data. These approaches define a forward diffusion process for transforming data into noise and a backward denoising process for sampling data from noise. Unfortunately, the generation process of current denoising diffusion models is notoriously slow due to the lengthy iterative noise estimations, which rely on cumbersome neural networks. It prevents the diffusion models from being widely deployed, especially on edge devices. Previous works accelerate the generation process of diffusion model (DM) via finding shorter yet effective sampling trajectories. However, they overlook the cost of noise estimation with a heavy network in every iteration. In this work, we accelerate generation from the perspective of compressing the noise estimation network. Due to the difficulty of retraining DMs, we exclude mainstream training-aware compression paradigms and introduce post-training quantization (PTQ) into DM acceleration. However, the output distributions of noise estimation networks change with time-step, making previous PTQ methods fail in DMs since they are designed for single-time step scenarios. To devise a DM-specific PTQ method, we explore PTQ on DM in three aspects: quantized operations, calibration dataset, and calibration metric. We summarize and use several observations derived from all-inclusive investigations to formulate our method, which especially targets the unique multi-time-step structure of DMs. Experimentally, our method can directly quantize full-precision DMs into 8-bit models while maintaining or even improving their performance in a training-free manner. Importantly, our method can serve as a plug-and-play module on other fast-sampling methods, e.g., DDIM. The code is available at https://github.com/42Shawn/PTQ4DM .",
      "meta_data": {
        "arxiv_id": "2211.15736v3",
        "authors": [
          "Yuzhang Shang",
          "Zhihang Yuan",
          "Bin Xie",
          "Bingzhe Wu",
          "Yan Yan"
        ],
        "published_date": "2022-11-28T19:33:39Z",
        "pdf_url": "https://arxiv.org/pdf/2211.15736v3.pdf"
      }
    },
    {
      "title": "A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models",
      "abstract": "Diffusion models have shown remarkable performance in generation problems over various domains including images, videos, text, and audio. A practical bottleneck of diffusion models is their sampling speed, due to the repeated evaluation of score estimation networks during the inference. In this work, we propose a novel framework capable of adaptively allocating compute required for the score estimation, thereby reducing the overall sampling time of diffusion models. We observe that the amount of computation required for the score estimation may vary along the time step for which the score is estimated. Based on this observation, we propose an early-exiting scheme, where we skip the subset of parameters in the score estimation network during the inference, based on a time-dependent exit schedule. Using the diffusion models for image synthesis, we show that our method could significantly improve the sampling throughput of the diffusion models without compromising image quality. Furthermore, we also demonstrate that our method seamlessly integrates with various types of solvers for faster sampling, capitalizing on their compatibility to enhance overall efficiency. The source code and our experiments are available at \\url{https://github.com/taehong-moon/ee-diffusion}",
      "meta_data": {
        "arxiv_id": "2408.05927v1",
        "authors": [
          "Taehong Moon",
          "Moonseok Choi",
          "EungGu Yun",
          "Jongmin Yoon",
          "Gayoung Lee",
          "Jaewoong Cho",
          "Juho Lee"
        ],
        "published_date": "2024-08-12T05:33:45Z",
        "pdf_url": "https://arxiv.org/pdf/2408.05927v1.pdf"
      }
    },
    {
      "title": "Multistep Distillation of Diffusion Models via Moment Matching",
      "abstract": "We present a new method for making diffusion models faster to sample. The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory. Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching. By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset. We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers.",
      "meta_data": {
        "arxiv_id": "2406.04103v1",
        "authors": [
          "Tim Salimans",
          "Thomas Mensink",
          "Jonathan Heek",
          "Emiel Hoogeboom"
        ],
        "published_date": "2024-06-06T14:20:21Z",
        "pdf_url": "https://arxiv.org/pdf/2406.04103v1.pdf"
      }
    },
    {
      "title": "Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference",
      "abstract": "One of the main drawback of diffusion models is the slow inference time for image generation. Among the most successful approaches to addressing this problem are distillation methods. However, these methods require considerable computational resources. In this paper, we take another approach to diffusion model acceleration. We conduct a comprehensive study of the UNet encoder and empirically analyze the encoder features. This provides insights regarding their changes during the inference process. In particular, we find that encoder features change minimally, whereas the decoder features exhibit substantial variations across different time-steps. This insight motivates us to omit encoder computation at certain adjacent time-steps and reuse encoder features of previous time-steps as input to the decoder in multiple time-steps. Importantly, this allows us to perform decoder computation in parallel, further accelerating the denoising process. Additionally, we introduce a prior noise injection method to improve the texture details in the generated image. Besides the standard text-to-image task, we also validate our approach on other tasks: text-to-video, personalized generation and reference-guided generation. Without utilizing any knowledge distillation technique, our approach accelerates both the Stable Diffusion (SD) and DeepFloyd-IF model sampling by 41$\\%$ and 24$\\%$ respectively, and DiT model sampling by 34$\\%$, while maintaining high-quality generation performance.",
      "meta_data": {
        "arxiv_id": "2312.09608v2",
        "authors": [
          "Senmao Li",
          "Taihang Hu",
          "Joost van de Weijer",
          "Fahad Shahbaz Khan",
          "Tao Liu",
          "Linxuan Li",
          "Shiqi Yang",
          "Yaxing Wang",
          "Ming-Ming Cheng",
          "Jian Yang"
        ],
        "published_date": "2023-12-15T08:46:43Z",
        "pdf_url": "https://arxiv.org/pdf/2312.09608v2.pdf"
      }
    },
    {
      "title": "Leveraging Early-Stage Robustness in Diffusion Models for Efficient and High-Quality Image Synthesis"
    },
    {
      "title": "Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy",
      "abstract": "Diffusion models have recently achieved great success in the synthesis of high-quality images and videos. However, the existing denoising techniques in diffusion models are commonly based on step-by-step noise predictions, which suffers from high computation cost, resulting in a prohibitive latency for interactive applications. In this paper, we propose AdaptiveDiffusion to relieve this bottleneck by adaptively reducing the noise prediction steps during the denoising process. Our method considers the potential of skipping as many noise prediction steps as possible while keeping the final denoised results identical to the original full-step ones. Specifically, the skipping strategy is guided by the third-order latent difference that indicates the stability between timesteps during the denoising process, which benefits the reusing of previous noise prediction results. Extensive experiments on image and video diffusion models demonstrate that our method can significantly speed up the denoising process while generating identical results to the original process, achieving up to an average 2~5x speedup without quality degradation.",
      "meta_data": {
        "arxiv_id": "2410.09873v1",
        "authors": [
          "Hancheng Ye",
          "Jiakang Yuan",
          "Renqiu Xia",
          "Xiangchao Yan",
          "Tao Chen",
          "Junchi Yan",
          "Botian Shi",
          "Bo Zhang"
        ],
        "published_date": "2024-10-13T15:19:18Z",
        "pdf_url": "https://arxiv.org/pdf/2410.09873v1.pdf"
      }
    },
    {
      "title": "Post-Training Quantization on Diffusion Models",
      "abstract": "Denoising diffusion (score-based) generative models have recently achieved significant accomplishments in generating realistic and diverse data. These approaches define a forward diffusion process for transforming data into noise and a backward denoising process for sampling data from noise. Unfortunately, the generation process of current denoising diffusion models is notoriously slow due to the lengthy iterative noise estimations, which rely on cumbersome neural networks. It prevents the diffusion models from being widely deployed, especially on edge devices. Previous works accelerate the generation process of diffusion model (DM) via finding shorter yet effective sampling trajectories. However, they overlook the cost of noise estimation with a heavy network in every iteration. In this work, we accelerate generation from the perspective of compressing the noise estimation network. Due to the difficulty of retraining DMs, we exclude mainstream training-aware compression paradigms and introduce post-training quantization (PTQ) into DM acceleration. However, the output distributions of noise estimation networks change with time-step, making previous PTQ methods fail in DMs since they are designed for single-time step scenarios. To devise a DM-specific PTQ method, we explore PTQ on DM in three aspects: quantized operations, calibration dataset, and calibration metric. We summarize and use several observations derived from all-inclusive investigations to formulate our method, which especially targets the unique multi-time-step structure of DMs. Experimentally, our method can directly quantize full-precision DMs into 8-bit models while maintaining or even improving their performance in a training-free manner. Importantly, our method can serve as a plug-and-play module on other fast-sampling methods, e.g., DDIM. The code is available at https://github.com/42Shawn/PTQ4DM .",
      "meta_data": {
        "arxiv_id": "2211.15736v3",
        "authors": [
          "Yuzhang Shang",
          "Zhihang Yuan",
          "Bin Xie",
          "Bingzhe Wu",
          "Yan Yan"
        ],
        "published_date": "2022-11-28T19:33:39Z",
        "pdf_url": "https://arxiv.org/pdf/2211.15736v3.pdf"
      }
    },
    {
      "title": "Fast Sampling of Diffusion Models via Operator Learning",
      "abstract": "Diffusion models have found widespread adoption in various areas. However, their sampling process is slow because it requires hundreds to thousands of network evaluations to emulate a continuous process defined by differential equations. In this work, we use neural operators, an efficient method to solve the probability flow differential equations, to accelerate the sampling process of diffusion models. Compared to other fast sampling methods that have a sequential nature, we are the first to propose a parallel decoding method that generates images with only one model forward pass. We propose diffusion model sampling with neural operator (DSNO) that maps the initial condition, i.e., Gaussian distribution, to the continuous-time solution trajectory of the reverse diffusion process. To model the temporal correlations along the trajectory, we introduce temporal convolution layers that are parameterized in the Fourier space into the given diffusion model backbone. We show our method achieves state-of-the-art FID of 3.78 for CIFAR-10 and 7.83 for ImageNet-64 in the one-model-evaluation setting.",
      "meta_data": {
        "arxiv_id": "2211.13449v3",
        "authors": [
          "Hongkai Zheng",
          "Weili Nie",
          "Arash Vahdat",
          "Kamyar Azizzadenesheli",
          "Anima Anandkumar"
        ],
        "published_date": "2022-11-24T07:30:27Z",
        "pdf_url": "https://arxiv.org/pdf/2211.13449v3.pdf"
      }
    },
    {
      "title": "Denoising Diffusion Implicit Models",
      "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.",
      "meta_data": {
        "arxiv_id": "2010.02502v4",
        "authors": [
          "Jiaming Song",
          "Chenlin Meng",
          "Stefano Ermon"
        ],
        "published_date": "2020-10-06T06:15:51Z",
        "pdf_url": "https://arxiv.org/pdf/2010.02502v4.pdf"
      }
    },
    {
      "title": "gDDIM: Generalized denoising diffusion implicit models",
      "abstract": "Our goal is to extend the denoising diffusion implicit model (DDIM) to general diffusion models~(DMs) besides isotropic diffusions. Instead of constructing a non-Markov noising process as in the original DDIM, we examine the mechanism of DDIM from a numerical perspective. We discover that the DDIM can be obtained by using some specific approximations of the score when solving the corresponding stochastic differential equation. We present an interpretation of the accelerating effects of DDIM that also explains the advantages of a deterministic sampling scheme over the stochastic one for fast sampling. Building on this insight, we extend DDIM to general DMs, coined generalized DDIM (gDDIM), with a small but delicate modification in parameterizing the score network. We validate gDDIM in two non-isotropic DMs: Blurring diffusion model (BDM) and Critically-damped Langevin diffusion model (CLD). We observe more than 20 times acceleration in BDM. In the CLD, a diffusion model by augmenting the diffusion process with velocity, our algorithm achieves an FID score of 2.26, on CIFAR10, with only 50 number of score function evaluations~(NFEs) and an FID score of 2.86 with only 27 NFEs. Code is available at https://github.com/qsh-zh/gDDIM",
      "meta_data": {
        "arxiv_id": "2206.05564v2",
        "authors": [
          "Qinsheng Zhang",
          "Molei Tao",
          "Yongxin Chen"
        ],
        "published_date": "2022-06-11T16:57:23Z",
        "pdf_url": "https://arxiv.org/pdf/2206.05564v2.pdf"
      }
    },
    {
      "title": "Multistep Distillation of Diffusion Models via Moment Matching",
      "abstract": "We present a new method for making diffusion models faster to sample. The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory. Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching. By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset. We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers.",
      "meta_data": {
        "arxiv_id": "2406.04103v1",
        "authors": [
          "Tim Salimans",
          "Thomas Mensink",
          "Jonathan Heek",
          "Emiel Hoogeboom"
        ],
        "published_date": "2024-06-06T14:20:21Z",
        "pdf_url": "https://arxiv.org/pdf/2406.04103v1.pdf"
      }
    },
    {
      "title": "Semi-Implicit Denoising Diffusion Models (SIDDMs)",
      "abstract": "Despite the proliferation of generative models, achieving fast sampling during inference without compromising sample diversity and quality remains challenging. Existing models such as Denoising Diffusion Probabilistic Models (DDPM) deliver high-quality, diverse samples but are slowed by an inherently high number of iterative steps. The Denoising Diffusion Generative Adversarial Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN model for larger jumps in the diffusion process. However, DDGAN encountered scalability limitations when applied to large datasets. To address these limitations, we introduce a novel approach that tackles the problem by matching implicit and explicit factors. More specifically, our approach involves utilizing an implicit model to match the marginal distributions of noisy data and the explicit conditional distribution of the forward diffusion. This combination allows us to effectively match the joint denoising distributions. Unlike DDPM but similar to DDGAN, we do not enforce a parametric distribution for the reverse step, enabling us to take large steps during inference. Similar to the DDPM but unlike DDGAN, we take advantage of the exact form of the diffusion process. We demonstrate that our proposed method obtains comparable generative performance to diffusion-based models and vastly superior results to models with a small number of sampling steps.",
      "meta_data": {
        "arxiv_id": "2306.12511v3",
        "authors": [
          "Yanwu Xu",
          "Mingming Gong",
          "Shaoan Xie",
          "Wei Wei",
          "Matthias Grundmann",
          "Kayhan Batmanghelich",
          "Tingbo Hou"
        ],
        "published_date": "2023-06-21T18:49:22Z",
        "pdf_url": "https://arxiv.org/pdf/2306.12511v3.pdf"
      }
    },
    {
      "title": "On Distillation of Guided Diffusion Models",
      "abstract": "Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALLE-2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet 64x64 and CIFAR-10, achieving FID/IS scores comparable to that of the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space (e.g., Stable Diffusion), our approach is able to generate high-fidelity images using as few as 1 to 4 denoising steps, accelerating inference by at least 10-fold compared to existing methods on ImageNet 256x256 and LAION datasets. We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2-4 denoising steps.",
      "meta_data": {
        "arxiv_id": "2210.03142v3",
        "authors": [
          "Chenlin Meng",
          "Robin Rombach",
          "Ruiqi Gao",
          "Diederik P. Kingma",
          "Stefano Ermon",
          "Jonathan Ho",
          "Tim Salimans"
        ],
        "published_date": "2022-10-06T18:03:56Z",
        "pdf_url": "https://arxiv.org/pdf/2210.03142v3.pdf"
      }
    },
    {
      "title": "EM Distillation for One-step Diffusion Models",
      "abstract": "While diffusion models can learn complex distributions, sampling requires a computationally expensive iterative process. Existing distillation methods enable efficient sampling, but have notable limitations, such as performance degradation with very few sampling steps, reliance on training data access, or mode-seeking optimization that may fail to capture the full distribution. We propose EM Distillation (EMD), a maximum likelihood-based approach that distills a diffusion model to a one-step generator model with minimal loss of perceptual quality. Our approach is derived through the lens of Expectation-Maximization (EM), where the generator parameters are updated using samples from the joint distribution of the diffusion teacher prior and inferred generator latents. We develop a reparametrized sampling scheme and a noise cancellation technique that together stabilizes the distillation process. We further reveal an interesting connection of our method with existing methods that minimize mode-seeking KL. EMD outperforms existing one-step generative methods in terms of FID scores on ImageNet-64 and ImageNet-128, and compares favorably with prior work on distilling text-to-image diffusion models.",
      "meta_data": {
        "arxiv_id": "2405.16852v2",
        "authors": [
          "Sirui Xie",
          "Zhisheng Xiao",
          "Diederik P Kingma",
          "Tingbo Hou",
          "Ying Nian Wu",
          "Kevin Patrick Murphy",
          "Tim Salimans",
          "Ben Poole",
          "Ruiqi Gao"
        ],
        "published_date": "2024-05-27T05:55:22Z",
        "pdf_url": "https://arxiv.org/pdf/2405.16852v2.pdf"
      }
    },
    {
      "title": "Multistep Distillation of Diffusion Models via Moment Matching",
      "abstract": "We present a new method for making diffusion models faster to sample. The method distills many-step diffusion models into few-step models by matching conditional expectations of the clean data given noisy data along the sampling trajectory. Our approach extends recently proposed one-step methods to the multi-step case, and provides a new perspective by interpreting these approaches in terms of moment matching. By using up to 8 sampling steps, we obtain distilled models that outperform not only their one-step versions but also their original many-step teacher models, obtaining new state-of-the-art results on the Imagenet dataset. We also show promising results on a large text-to-image model where we achieve fast generation of high resolution images directly in image space, without needing autoencoders or upsamplers.",
      "meta_data": {
        "arxiv_id": "2406.04103v1",
        "authors": [
          "Tim Salimans",
          "Thomas Mensink",
          "Jonathan Heek",
          "Emiel Hoogeboom"
        ],
        "published_date": "2024-06-06T14:20:21Z",
        "pdf_url": "https://arxiv.org/pdf/2406.04103v1.pdf"
      }
    },
    {
      "title": "Data-free Distillation of Diffusion Models with Bootstrapping",
      "abstract": "Diffusion models have demonstrated excellent potential for generating diverse images. However, their performance often suffers from slow generation due to iterative denoising. Knowledge distillation has been recently proposed as a remedy that can reduce the number of inference steps to one or a few without significant quality degradation. However, existing distillation methods either require significant amounts of offline computation for generating synthetic training data from the teacher model or need to perform expensive online learning with the help of real data. In this work, we present a novel technique called BOOT, that overcomes these limitations with an efficient data-free distillation algorithm. The core idea is to learn a time-conditioned model that predicts the output of a pre-trained diffusion model teacher given any time step. Such a model can be efficiently trained based on bootstrapping from two consecutive sampled steps. Furthermore, our method can be easily adapted to large-scale text-to-image diffusion models, which are challenging for conventional methods given the fact that the training sets are often large and difficult to access. We demonstrate the effectiveness of our approach on several benchmark datasets in the DDIM setting, achieving comparable generation quality while being orders of magnitude faster than the diffusion teacher. The text-to-image results show that the proposed approach is able to handle highly complex distributions, shedding light on more efficient generative modeling.",
      "meta_data": {
        "arxiv_id": "2306.05544v1",
        "authors": [
          "Jiatao Gu",
          "Shuangfei Zhai",
          "Yizhe Zhang",
          "Lingjie Liu",
          "Josh Susskind"
        ],
        "published_date": "2023-06-08T20:30:55Z",
        "pdf_url": "https://arxiv.org/pdf/2306.05544v1.pdf"
      }
    },
    {
      "title": "Plug-and-Play Diffusion Distillation",
      "abstract": "Diffusion models have shown tremendous results in image generation. However, due to the iterative nature of the diffusion process and its reliance on classifier-free guidance, inference times are slow. In this paper, we propose a new distillation approach for guided diffusion models in which an external lightweight guide model is trained while the original text-to-image model remains frozen. We show that our method reduces the inference computation of classifier-free guided latent-space diffusion models by almost half, and only requires 1\\% trainable parameters of the base model. Furthermore, once trained, our guide model can be applied to various fine-tuned, domain-specific versions of the base diffusion model without the need for additional training: this \"plug-and-play\" functionality drastically improves inference computation while maintaining the visual fidelity of generated images. Empirically, we show that our approach is able to produce visually appealing results and achieve a comparable FID score to the teacher with as few as 8 to 16 steps.",
      "meta_data": {
        "arxiv_id": "2406.01954v2",
        "authors": [
          "Yi-Ting Hsiao",
          "Siavash Khodadadeh",
          "Kevin Duarte",
          "Wei-An Lin",
          "Hui Qu",
          "Mingi Kwon",
          "Ratheesh Kalarot"
        ],
        "published_date": "2024-06-04T04:22:47Z",
        "pdf_url": "https://arxiv.org/pdf/2406.01954v2.pdf"
      }
    },
    {
      "title": "One-step Diffusion with Distribution Matching Distillation",
      "abstract": "Diffusion models generate high-quality images but require dozens of forward passes. We introduce Distribution Matching Distillation (DMD), a procedure to transform a diffusion model into a one-step image generator with minimal impact on image quality. We enforce the one-step image generator match the diffusion model at distribution level, by minimizing an approximate KL divergence whose gradient can be expressed as the difference between 2 score functions, one of the target distribution and the other of the synthetic distribution being produced by our one-step generator. The score functions are parameterized as two diffusion models trained separately on each distribution. Combined with a simple regression loss matching the large-scale structure of the multi-step diffusion outputs, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. Utilizing FP16 inference, our model generates images at 20 FPS on modern hardware.",
      "meta_data": {
        "arxiv_id": "2311.18828v4",
        "authors": [
          "Tianwei Yin",
          "Michaël Gharbi",
          "Richard Zhang",
          "Eli Shechtman",
          "Fredo Durand",
          "William T. Freeman",
          "Taesung Park"
        ],
        "published_date": "2023-11-30T18:59:20Z",
        "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024",
        "pdf_url": "https://arxiv.org/pdf/2311.18828v4.pdf"
      }
    }
  ]
}